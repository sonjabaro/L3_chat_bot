{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langdetect\n",
    "# pip install sentencepiece\n",
    "# pip install boto3\n",
    "# pip install awscli\n",
    "# pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import tempfile\n",
    "from langdetect import detect\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import re\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from functions file\n",
    "\n",
    "from functions_mm import handle_query, transcribe_audio_original, submit_question, polly_text_to_speech, translate, translate_and_speech, clear_inputs, voice_map, language_map, default_language, languages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "# Store the API key in a variable.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://d38cfed7bcd225f5fe.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d38cfed7bcd225f5fe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78af351a5b5d4a2a9e060207c0641fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mered\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-hi-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b36c313d1b4a14b656170dbc3fa2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba296ba5a91b4519adef320f290842a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/813k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd94cddc53654d41854399d62b5e6063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0866237bc73743c19a472682f5be360e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523cad3e1f604ea5930a7f9be22cefa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/304M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653f366ea12a43bc923a92d4d9caeae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an error occured: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 411, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 69, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 689, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\routing.py\", line 77, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\responses.py\", line 351, in __call__\n",
      "    await send(\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 549, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mered\\OneDrive\\Documents\\AI_Bootcamp\\1_DataClass\\L3_chat_bot\\functions_mm.py\", line 300, in translate_and_speech\n",
      "    detected_lang = [key for key, value in language_map.items() if value == detected_lang_code][0]\n",
      "                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "# Diabetes Chatbot\n",
    "#### Step 1: Record your audio OR input text (NOT both!)\n",
    "#### Step 2: Would you like response in new language? Choose your language  \n",
    "#### Step 3: Submit question\n",
    "#### Step 4: Translate response\n",
    "#### Step 5: Clear inputs and start fresh\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as app2:\n",
    "    \n",
    "    with gr.Row():\n",
    "        gr.Markdown(instructions)\n",
    "        \n",
    "    with gr.Row():\n",
    "        input_audio = gr.Audio(\n",
    "            label=\"Ask a question about Diabetes, then click 'Transcribe audio\",\n",
    "            type=\"filepath\")\n",
    "        language_dropdown = gr.Dropdown(label=\"Click the middle of the dropdown bar to select translation language\",\n",
    "                                        choices=list(language_map.keys()), value=default_language, type='value')\n",
    "        \n",
    "    with gr.Row():\n",
    "        transcribe_button = gr.Button(\"Transcribe audio\")\n",
    "        submit_button = gr.Button(\"Submit your question\")\n",
    "        translate_button = gr.Button(\"Translate the response\")\n",
    "        clear_button = gr.Button(\"Clear All\")\n",
    "    \n",
    "    #Divide the screen horizontally into 2 columns\n",
    "    with gr.Row():\n",
    "            #This column will be on the left side of screen\n",
    "            with gr.Column():\n",
    "                query_text = gr.Textbox(label=\"Type your question here. If there is audio recorded AND question text, app will submit question text. Click transcribe button to populate with audio text\")\n",
    "                # output_original_speech = gr.Audio(label=\"Text to speech here\")\n",
    "            \n",
    "            with gr.Column():\n",
    "                response_text = gr.Textbox(label=\"Chatbot response\")\n",
    "                response_speech = gr.Audio(label=\"Chatbot response speech\")    \n",
    "        \n",
    "            #This column will be on the right side of screen\n",
    "            with gr.Column():    \n",
    "                output_translated = gr.Textbox(label=\"Translated text\")\n",
    "                output_translated_speech = gr.Audio(label=\"Translated speech\")\n",
    "\n",
    "    # Audio transcription\n",
    "    transcribe_button.click(\n",
    "        fn=transcribe_audio_original,\n",
    "        inputs=[input_audio],\n",
    "        outputs=[query_text]\n",
    "    )\n",
    "    \n",
    "    submit_button.click(\n",
    "        fn=submit_question,\n",
    "        inputs=[input_audio, query_text, language_dropdown],\n",
    "        outputs=[response_text, response_speech]\n",
    "    )\n",
    "        \n",
    "    # Translation\n",
    "    translate_button.click(\n",
    "        fn=translate_and_speech,\n",
    "        inputs=[response_text, language_dropdown],\n",
    "        outputs=[output_translated, output_translated_speech]\n",
    "    )\n",
    "        \n",
    "    #Clearing all inputs and outputs\n",
    "    clear_button.click(\n",
    "    fn=clear_inputs,\n",
    "    inputs=[],\n",
    "    outputs=[input_audio, query_text, response_text, response_speech, output_translated, output_translated_speech]\n",
    "    )\n",
    "\n",
    "app2.launch(show_error=True, share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
